{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def f(x):\n",
    "    return x[0]**4 + x[1]**4 + x[2]**4 - x[0]**3*x[2] - 12*x[0]*x[2]**2 + x[0]*x[1]*x[2] - \\\n",
    "           3*x[1]**3 - 4*x[0]*x[1] + x[3]**2 + 9*x[1] - 3*x[3] + 4\n",
    "  \n",
    "def gradf(x):\n",
    "    return np.array([\n",
    "        4*x[0]**3 - 3*x[0]**2*x[2] - 12*x[2]**2 + x[1]*x[2] - 4*x[1],\n",
    "        4*x[1]**3 + x[0]*x[2] - 9*x[1]**2 - 4*x[0] + 9,\n",
    "        4*x[2]**3 - x[0]**3 - 24*x[0]*x[2] + x[0]*x[1],\n",
    "        2*x[3] - 3])\n",
    "\n",
    "def hessf(x):\n",
    "    return np.array([\n",
    "    [12*x[0]**2-6*x[0]*x[1],  x[2]-4,             -3*x[0]**2-24*x[2]+x[1], 0],\n",
    "    [x[2]-4,                  12*x[1]**2-18*x[1], x[0],                    0],\n",
    "    [-3*x[0]**2-24*x[2]+x[1], x[0],               12*x[2]**2-24*x[0],      0],\n",
    "    [0,                       0,                  0,                       2]])\n",
    "\n",
    "def multiVariableHalfOpen(f, x, d, T):\n",
    "    '''\n",
    "    INPUT\n",
    "        f: multivariable function to minimise\n",
    "        x: starting point\n",
    "        d: direction vector\n",
    "        T: upper bound increment parameter\n",
    "    OUTPUT\n",
    "        a: lower bound on the location of minimum of f in direction d from x\n",
    "        b: upper bound on the location of minimum of f in direction d from x\n",
    "    '''\n",
    "    k = 1\n",
    " \n",
    "    p = x\n",
    "    q = x + T*d\n",
    "\n",
    "    fp = f(p)\n",
    "    fq = f(q)\n",
    "\n",
    "    \n",
    "    while fp > fq:\n",
    "        k += 1\n",
    "        p = q\n",
    "        fp = fq\n",
    "        q = p + (2**(k-1))*T*d\n",
    "        fq = f(q)\n",
    "            \n",
    "    if k == 1:\n",
    "        a = 0\n",
    "        b = T\n",
    "\n",
    "    elif k == 2:\n",
    "        a = 0\n",
    "        b = 3*T\n",
    "\n",
    "    else:\n",
    "        u = np.arange(0,k,1)\n",
    "        v = np.arange(0,k-2,1)\n",
    "\n",
    "        a = T*sum((2*np.ones(k-2))**v)       \n",
    "        b = T*sum((2*np.ones(k))**u) \n",
    "        \n",
    "    return a, b\n",
    "\n",
    "def multiVariableGoldenSectionSearch(f, a, b, tolerance, x, d):\n",
    "    '''\n",
    "    performs golden section search for finding minimum of f along the\n",
    "    direction d, starting at x, where the minimum has upper and lower bound [a, b]\n",
    "    '''\n",
    "    if b <= a:\n",
    "        raise ValueError('b must be strictly greater than a')\n",
    "    if tolerance <= 0:\n",
    "        raise ValueError('tolerance must be strictly positive')\n",
    "\n",
    "    # Begin the Golden Search algorithm\n",
    "\n",
    "    gamma = (np.sqrt(5) - 1)/2 \n",
    "    k = 1\n",
    "\n",
    "    p = b - gamma*(b-a)\n",
    "    q = a + gamma*(b-a)\n",
    "\n",
    "    fp = f(x + p*d)\n",
    "    fq = f(x + q*d)\n",
    "\n",
    "    while b-a >= 2*tolerance:\n",
    "        k += 1\n",
    "\n",
    "        if fp <= fq:\n",
    "            b = q\n",
    "            q = p\n",
    "            fq = fp\n",
    "            p = b - gamma*(b-a)\n",
    "            fp = f(x + p*d)\n",
    "\n",
    "        else:\n",
    "            a = p\n",
    "            p = q\n",
    "            fp = fq\n",
    "            q = a + gamma*(b-a)\n",
    "            fq = f(x + q*d)\n",
    "\n",
    "    # Midpoint of the final interval\n",
    "    minEstimate = (a+b)/2\n",
    "    fminEstimate = f(x + minEstimate*d)  \n",
    "    return minEstimate, fminEstimate\n",
    "\n",
    "def BFGS(f, gradf, x0, H0, tolerance1, tolerance2, T):\n",
    "    '''\n",
    "    INPUT\n",
    "        f:          the multivariable function to minimise\n",
    "        gradf:      function which returns the gradient vector of f evaluated at x\n",
    "        x0:         the starting iterate\n",
    "        tolerance1: tolerance for stopping criterion of steepest descent method\n",
    "        tolerance2: tolerance for stopping criterion of line minimisation\n",
    "        T:          parameter used by the \"improved algorithm for finding an upper bound for the minimum\" along \n",
    "                    each given descent direction\n",
    "    OUTPUT\n",
    "        xminEstimate: estimate of the minimum\n",
    "        fminEstimate: the value of f at xminEstimate\n",
    "        k:            iteration counter\n",
    "    '''\n",
    "    k = 0\n",
    "    iteration_number = 0\n",
    "\n",
    "    xk = x0\n",
    "    xk_old = x0\n",
    "    H_old = H0\n",
    "    \n",
    "    while norm(gradf(xk)) >= tolerance1:\n",
    "        iteration_number += 1  \n",
    "        \n",
    "        # Correction if det H_old gets too large or small\n",
    "        H_old /= np.amax(H_old) \n",
    "\n",
    "        # Get dk as a row vector\n",
    "        dk = -H_old.dot(gradf(xk))\n",
    "        # minimise f with respect to t in the direction dk\n",
    "\n",
    "        # (1) find upper and lower bound, [a,b], for the stepsize t\n",
    "        a, b = multiVariableHalfOpen(f, xk, dk, T)\n",
    "\n",
    "        # (2) use golden section algorithm to estimate the stepsize t in [a,b] which minimises f in the direction dk from xk\n",
    "        tmin, fmin = multiVariableGoldenSectionSearch(f, a, b, tolerance2, xk, dk);\n",
    "\n",
    "        k += 1\n",
    "\n",
    "        xk += tmin*dk\n",
    "        xk_new = xk_old +tmin*dk\n",
    "\n",
    "        sk=(xk_new - xk_old).T\n",
    "        gk= (gradf(xk_new)-gradf(xk_old)).T\n",
    "        rk=(H_old.dot(gk))/(sk.dot(gk))\n",
    "        \n",
    "        H_new = H_old + (1 + rk.dot(gk))/(sk.dot(gk))*np.outer(sk, sk) - np.outer(sk, rk) - np.outer(rk, sk)\n",
    "        \n",
    "        xk_old = xk_new\n",
    "        H_old = H_new\n",
    "\n",
    "    xminEstimate = xk\n",
    "    fminEstimate = f(xminEstimate)\n",
    "    return xminEstimate, fminEstimate, k\n",
    "\n",
    "def NewtonMethod(f, gradf, hessf, x0, tolerance1, tolerance2, T):\n",
    "    '''\n",
    "    INPUT\n",
    "        f:          the multivariable function to minimise\n",
    "        gradf:      function which returns the gradient vector of f evaluated at x \n",
    "        x0:         the starting iterate\n",
    "        tolerance1: tolerance for stopping criterion of steepest descent method\n",
    "        tolerance2: tolerance for stopping criterion of line minimisation\n",
    "        T:          parameter used by the \"improved algorithm for finding an upper bound for the minimum\" along \n",
    "                    each given descent direction\n",
    "    OUTPUT\n",
    "        xminEstimate: estimate of the minimum\n",
    "        fminEstimate: the value of f at xminEstimate\n",
    "        k:            iteration counter\n",
    "    '''\n",
    "    k = 0\n",
    "    xk = x0\n",
    "    while norm(gradf(xk)) >= tolerance1:\n",
    "        gradf(xk)\n",
    "        Hessian = hessf(xk)\n",
    "        \n",
    "        # Correction if det Hessian gets too large or small\n",
    "        Hessian /= np.amax(Hessian) \n",
    "\n",
    "        # Checks to see if the Hessian is positive definite\n",
    "        if np.all(np.linalg.eigvals(Hessian) > 0):\n",
    "            # the Newton direction - as a row vector\n",
    "            dk = -(np.linalg.inv(Hessian).dot(gradf(xk).T)).T\n",
    "        else:\n",
    "            # the steepest descent direction.\n",
    "            dk = -gradf(xk)\n",
    "                   \n",
    "        # Minimise f with respect to t in the direction dk\n",
    "                   \n",
    "        # (1) find upper and lower bound,[a,b],for the stepsize t \n",
    "        a, b = multiVariableHalfOpen(f, xk, dk, T)\n",
    "        \n",
    "        # (2) use golden section algorithm to estimate the stepsize t in [a,b] which minimises f in the direction dk from xk\n",
    "        tmin, fmin = multiVariableGoldenSectionSearch(f, a, b, tolerance2, xk, dk)\n",
    "                   \n",
    "        k += 1\n",
    "        xk += tmin*dk\n",
    "                   \n",
    "    xminEstimate = xk\n",
    "    fminEstimate = f(xminEstimate)\n",
    "    return xminEstimate, fminEstimate, k\n",
    "\n",
    "def steepestDescentMethod(f, gradf, x0, tolerance1, tolerance2, T):\n",
    "    '''\n",
    "    INPUT\n",
    "        f:          the multivariable function to minimise\n",
    "        gradf:      function which returns the gradient vector of f evaluated at x \n",
    "        x0:         the starting iterate\n",
    "        tolerance1: tolerance for stopping criterion of steepest descent method\n",
    "        tolerance2: tolerance for stopping criterion of line minimisation\n",
    "        T:          parameter used by the \"improved algorithm for finding an upper bound for the minimum\" along \n",
    "                    each given descent direction\n",
    "    OUTPUT\n",
    "        xminEstimate: estimate of the minimum\n",
    "        fminEstimate: the value of f at xminEstimate\n",
    "        k:            iteration counter\n",
    "    '''\n",
    "    k = 0\n",
    "    xk = x0\n",
    "    \n",
    "    while norm(gradf(xk)) >= tolerance1:\n",
    "        # Steepest descent direction\n",
    "        dk = -gradf(xk)\n",
    "        \n",
    "        # Minimise f with respect to t in the direction dk\n",
    "        \n",
    "        # (1) find upper and lower bound, [a,b], for the stepsize t\n",
    "        a, b = multiVariableHalfOpen(f, xk, dk, T)\n",
    "\n",
    "        # (2) use golden section algorithm  to estimate the stepsize t in [a,b] which minimises f in the direction dk from xk\n",
    "        tmin, fmin = multiVariableGoldenSectionSearch(f, a, b, tolerance2, xk, dk)\n",
    "\n",
    "        k += 1\n",
    "        xk += tmin*dk\n",
    "        \n",
    "    xminEstimate = xk\n",
    "    fminEstimate = f(xminEstimate)\n",
    "    return xminEstimate, fminEstimate, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.48277244, -1.85425693,  9.00346308,  1.50010064]),\n",
       " -2316.4978114162277,\n",
       " 24)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BFGS(f, gradf, np.array([1.0, 2,3,4]), np.eye(4), 10**(-2), 10**(-5), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.48280486, -1.8542721 ,  9.00348825,  1.50019707]),\n",
       " -2316.4978114423793,\n",
       " 12)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NewtonMethod(f, gradf, hessf, np.array([1.0, 2,3,4]), 10**(-2), 10**(-5), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.48279833, -1.85426806,  9.00347676,  1.50392842]),\n",
       " -2316.4977960450055,\n",
       " 1661)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steepestDescentMethod(f, gradf, np.array([1.0, 2,3,4]), 10**(-2), 10**(-5), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
